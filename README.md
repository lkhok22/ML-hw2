# ML-hw2

------------------------პროექტის მიმოხილვა------------------
IEEE-CIS Fraud Detection არის kaggle-ის კონკურსი, რომლის მიზანია განისაზღვროს, არის თუ არა ონლაინ ტრანზაქცია გაყალბებული (isFraud ცვლადი) ტრანზაქციის მონაცემების მიხედვით.
მონაცემები გაყოფილია ორ ფაილად: identity და transaction, რომლებიც დაკავშირებულია TransactionID-ის მიხედვით. რისი გაერთიანებაც ჩვენით მოგვიწევს. ეს ფაილები მოიცავს როგორც რიცხვით,
ასევე კატეგორიულ ცვლადებსაც.


------------------------ჩემი მიდგომა------------------------
ჩემი მიდგომა იყო რომ დამეწყო რაც შეიძლება მარტივად და დრო და დრო გამეუმჯობესებინა მიდგომა, ამიტომ თავდაპირველად დავიწყე logistic regression-ით.
რაც შეეხება train-ის დაყოფას, თავიდან 70 10 20 (train validation test) დავყავი, მაგრამ საბოლოოდ ვალიდაციის გამოყენება ამხელა ცხრილზე დიდ დროს წაიღებდა და საჭიროდ აღარ ჩავთვალე და
სტანდარტულად 80 20 დავყავი.
# LOGISTIC REGRESSION
-cleaning
რადგანაც ამ მოდელს არ შეუძლია Null ცვლადების დამუშავება, ამიტომ ჯერ დავიწყე ყველაზე მარტივად რიცხვითების -999-ებით 
(ეს უკეთესია ვიდრე 0, იმიტომ რომ შეიძლება 0-ს ისედაც ქონდეს რაღაცა დანიშნულება და -999 უფრო უმნიშვნელოდ იქნება) ჩანაცვლებით და კატეგორიულში nan-ებით.
ამავდროულად დავდროპე 80%ზე მეტი null-ების შემცველი სვეტები. 

-feature engineering
მოცემული ფაილის გადახედვის შემდეგ შევნიშნე რომ უმეტესობა კატეგორიულ ცვლადს დიდი რაოდენობის განსხვავებული მონაცემები ქონდა, ამიტომ მხოლოდ one hot encodig
არ გამოდგებოდა რადგან საკმაოდ დიდი გახდებოდა ცხრილი. აამიტომ სემინარის მსგავსაც woe და one hot გავაერთიანე და 3 ზე მეტი განსხვავებულის შემდეგ woe ენკოდინგს ვუშვებდი.

-feature selection
ისეთ მონაცემებზე რომელიცაა V--- ან C--- (ან მსგავსი მონაცემებირომლების კონკრეტული დანიშულება ჩვენთვის უცნობია და უბრალოდ ბევრია) 
მონაცემების ერთმანეთთან კორელაციის heat map-ის და არამარტო(ნებისმიერი სხვა პლოტებითაც გამოჩნდებოდა) ნახაზებიდან კარგად ჩანს რამდენად დიდი რაოდენობაა ერთმანეთთან
მჭიდროდ კორელირებული და შესაბამისად ჩვენთვის არაფერი ახალის მომცემი. ამიტომ გადავწყვიტე კორელაციით ზედმეტი სვეტების დადროპვა.

და საბოლოოდ გავუშვი Logistic regressionze. ამავე მოდელზე ცვლილებებად მქონდა scaler-ის დამატება, რამაც ბევრად უკეთესი შედეგი დადო, და ასევე null-ების mean-ით და მოდათი ჩანაცვლებაც ვცადე, რამაც საბოლოოდ უარესი შედეგი მომცა.


# RANDOM FOREST

აქაც Logistic-ის მსგავად გავუშვი preprocessing თავიდან, მაგრამ რატომღაც auc score-ს 1-ს ვიღებდი ანუ რაღაც leak მქონდა, რაც ვერ გავასწორე, მაგრამ გადავწყვიტე რომ woe და one hot ხეებში ისედაც ზედმეტია, ამიტომ გამოვიყენე label encoding, და selection-ისთვის დავწერე ფუნქცია FeatureSelectorByImportance, რომელიც რამდენად მნიშვნელოვანია მაგის მიხედვით დროპავს ნაკლებად გამოსადეგ მონაცემების შემცველ სვეტებს.
ამან არც თუ ისე ცუდი შედეგი დადო, მაგრამ ჯერ ჯერობით Logistic-is საუკეთესო შედეგი სჯობნიდა.

# 
